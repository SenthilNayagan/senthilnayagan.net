---
title: "Essential Math for AI and ML: Introduction to Estimation Theory"
description: Estimation theory is like the detective work of data science. It equips us with methods to infer unknown parameters from observed data. Imagine estimating the average height in a city or the likelihood of a coin flip. Key techniques like Maximum Likelihood and Bayesian Estimation ensure our guesses are accurate and reliable. Used in fields from economics to engineering, estimation theory transforms raw data into meaningful insights. It’s the art and science of turning observations into actionable knowledge, guiding decisions in everyday life and complex industries alike. Dive into this fascinating world and become a data detective!
keywords: [estimation-theory, math-for-ai-ml, essential-math]
categories: [estimation-theory, math-for-ai-ml, essential-math]
coverImage: ./images/cover-image.webp
imageDescription: Old scholars in a grand study room, engaged in estimation.
imageCredits: Image generated by <b>DALL-E</b>.
lastUpdated: 2024-07-21
draft: true
---

{% include "toc.md" %}

# What is estimation theory?

Estimation theory is a branch of mathematics and statistics that deals with the process of estimating the value of a parameter<a href="#ref-1" class="reference-link" data-ref="ref-1"><sup id="back-to-1">1</sup></a> or a function from a set of noisy or uncertain data.  In simple terms, it's about making educated guesses<a href="#ref-2" class="reference-link" data-ref="ref-2"><sup id="back-to-2">2</sup></a> about things we can't directly measure or know for sure. In estimation theory, the goal is to find the best possible estimate of a parameter or a function based on a set of observations or measurements aka observed data. Imagine we're planning a road trip and we want to estimate how long it will take to get to our destination. We can use our knowledge of the distance, traffic patterns, and our own driving speed and skill to make an educated guess about the time it will take. This is an example of estimation theory in action!

Here are some more examples to help illustrate the concept:

- **Weather forecasting**: Meteorologists use historical data, current weather conditions, and computer models to estimate the probability of rain, temperature, and other weather patterns. They're making predictions about the future based on past data and current trends.
- **Stock market predictions**: Financial analysts use historical stock prices, economic indicators, and other data to estimate the future performance of a stock. They're trying to make informed guesses about which stocks will rise or fall in value.
- **Medical diagnosis**: Doctors use patient symptoms, medical history, and test results to estimate the likelihood of a particular disease or condition. They're making educated guesses about the patient's diagnosis based on available data.

By understanding estimation theory, we can make more informed decisions in our personal and professional life, and even develop our own predictive models to make better estimates about the world around us!

<!-- {% aside %}
**What does estimation theory offer?**

The estimation theory provide us with:

1. **Methods for estimating** the unknowns (model parameter, signals, etc.)
2. **Means for accessing** the "goodness" of the resulting estimates.
3. **Making confidence** statements about the true values.
{% endaside %} -->

## Approaches in estimation

1. **Minimizing the difference**
   - One common approach is to minimize the difference between the observed data and the predicted values, such as using least squares to find the line that best fits the data points.
2. **Maximizing the likelihood**
   - An alternative method is to maximize the likelihood that the observed data occurred given the estimated parameter or function. This technique is known as **maximum likelihood estimation** (**MLE**).

## Key concepts in estimation theory

There are several key concepts in estimation theory, including:

- **Estimator**: An estimator is a function that maps the observed data to an estimate of the parameter or function.
- **Bias**: Bias refers to the difference between the expected value of an estimator and the true value of the parameter or function.
- **Variance**: Variance refers to the spread or dispersion of an estimator around its expected value.
- **Consistency**: Consistency refers to the property of an estimator that its expected value converges to the true value of the parameter or function as the sample size increases.
- **Efficiency**: Efficiency refers to the property of an estimator that it has the smallest possible variance among all unbiased estimators.

## Different types of estimation

There are several types of estimation, including:

- **Point estimation**: Point estimation involves estimating a single value for the parameter or function.
- **Interval estimatio**n: Interval estimation involves estimating a range of values for the parameter or function.
- **Bayesian estimation**: Bayesian estimation involves using Bayes' theorem to update the probability distribution of the parameter or function based on the observed data.
- **Maximum likelihood estimation**: Maximum likelihood estimation involves finding the value of the parameter or function that maximizes the likelihood of the observed data.

## Common estimation techniques

Some common estimation techniques include:

- **Least squares estimation**: Least squares estimation involves minimizing the sum of the squared differences between the observed data and the predicted values.
- **Maximum likelihood estimation**: Maximum likelihood estimation involves finding the value of the parameter or function that maximizes the likelihood of the observed data.
- **Bayesian estimation**: Bayesian estimation involves using Bayes' theorem to update the probability distribution of the parameter or function based on the observed data.
- **Kalman filter estimation**: Kalman filter estimation involves using a recursive algorithm to estimate the state of a system based on noisy or uncertain data.

Having said that, estimation theory has many applications in various fields and is an essential tool for making informed decisions in many areas of science and engineering.

# Various mathematical notations uses in estimation theory

In estimation theory, various mathematical notations are used to represent data, parameters, estimators, and distributions. Here are some common notations along with their explanations:

## Data and samples

1. **x (Observed data, pronounced “x”)**:
   - A vector of observed/realized values from a sample.
   - Example: \\( x = (x_1, x_2, \ldots, x_n) \\)

2. **X (Random variables, pronounced “X”)**:
   - A vector of random variables.
   - Example: \\( X = (X_1, X_2, \ldots, X_n) \\)

## Parameters

1. **\\( \theta \\) (Parameter, pronounced “theta”)**:
   - A generic parameter of a probability distribution.
   - Example: \\( \theta \in \Theta \\)

2. **\\( \mu \\) (Mean, pronounced “mu”)**:
   - The mean of a distribution.

3. **\\( \sigma \\) (Standard deviation, pronounced “sigma”)**:
   - The standard deviation of a distribution.

4. **\\( \sigma^2 \\) (Variance, pronounced “sigma squared”)**:
   - The variance of a distribution.

5. **\\( p \\) (Probability, pronounced “p”)**:
   - The probability of success in a Bernoulli or binomial distribution.

## Estimators

1. **\\( \hat{\theta} \\) (Estimator of $ \theta $, pronounced “theta hat”)**:
   - An estimator of the parameter \\( \theta \\).
   - Example: \\( \hat{\theta} \\) is the sample mean used to estimate the population mean \\( \theta \\).

2. **\\( \hat{\mu} \\) (Estimator of $ \mu $, pronounced “mu hat”)**:
   - An estimator of the mean \\( \mu \\).

3. **\\( \hat{\sigma}^2 \\) (Estimator of $ \sigma^2 $, pronounced “sigma squared hat”)**:
   - An estimator of the variance \\( \sigma^2 \\).

## Probability and likelihood

1. **\\( P(X) \\) (Probability, pronounced “P of X”)**:
   - The probability of the random variable \\( X \\).

2. **\\( f(x|\theta) \\) (Probability density/mass function, pronounced “f of x given theta”)**:
   - The probability density function (PDF) or probability mass function (PMF) of \\( x \\) given parameter \\( \theta \\).
   - Example: \\( f(x|\theta) \\) for a normal distribution might be \\( f(x|\mu, \sigma^2) \\).

3. **\\( L(\theta|x) \\) (Likelihood function, pronounced “L of theta given x”)**:
   - The likelihood function of the parameter \\( \theta \\) given the observed data \\( x \\).
   - Example: \\( L(\theta|x) = f(x|\theta) \\) for the product of individual probabilities or densities.

4. **\\( \log L(\theta|x) \\) (Log-likelihood function, pronounced “log L of theta given x”)**:
   - The log-likelihood function, often used for simplification in calculations.

## Statistical inference

1. **\\( \mathcal{F} \\) (Family of distributions, pronounced “script F”)**:
   - A family of distributions.
   - Example: \\( \mathcal{F} = \{ f(x|\theta) | \theta \in \Theta \} \\)

2. **\\( \Theta \\) (Parameter space, pronounced “Theta”)**:
   - The parameter space, representing all possible values of \\( \theta \\).
   - Example: \\( \Theta = \mathbb{R} \\) for real-valued parameters.

3. **\\( E[X] \\) (Expected value, pronounced “E of X”)**:
   - The expected value of the random variable \\( X \\).

4. **\\( \text{Var}(X) \\) (Variance, pronounced “Variance of X”)**:
   - The variance of the random variable \\( X \\).

---

# Frequently asked questions (FAQs)

## What is central tendency in math?

Central tendency is a math concept that helps us understand how a set of numbers is clustered around a middle value. Think of it like a big group of people standing in a line, and we want to know where most of them are standing.

In math, central tendency is measured by three main values:

- **Mean**: It's an *average* of a set of values. It is calculated by adding up all the values and dividing by the number of values. It's a common measure for normally distributed data. Because the calculation includes all values, including the outliers, the mean is sensitive to extreme values (outliers).

- **Median**: It's the *middle value* in a dataset when the values are arranged in ascending or descending order. It's less sensitive to outliers and skewed data than the mean. It provides a better measure of central tendency for skewed distributions or datasets with outliers. Imagine a line that divides the group in half, and the median is the value that most people are standing on. For an odd number of values, the median is the value at the $ (n+1)/2th $ position. For an even number of values, the median is the average of the values at the $ n/2th $ and $ (n/2) + 1th $ positions. 

- **Mode**: The most common value that appears most frequently in a dataset. It's not affected by outliers. It's useful for categorical data and to identify the most common value in a dataset. Note that mode is not a good representation of the typical value because it's influenced by the frequency of the values. Imagine a big crowd of people, and the mode is the value that most people are standing on.

These three values help us understand how the group is clustered around a middle value. For example:

- If the mean is 10, it means that if we add up all the numbers and divide by the number of people, we get 10.
- If the median is 10, it means that half the people are standing on one side of the line and half are standing on the other side, with 10 being the middle value.
- If the mode is 10, it means that most people are standing on the value 10.

Central tendency is important in math because it helps us:

- Understand patterns and trends in data
- Make predictions and forecasts
- Compare different groups of data
- Identify outliers (numbers that are far away from the rest of the group)

So, in simple terms, central tendency is like finding the "average" or "middle" value of a group of numbers. It helps us understand how the numbers are clustered and what the typical value is.

## What are the measures of central tendency, and how do they differ from each other?

The commonly used measures of central tendency are the mean, median, and mode. 

**Mean**: The mean is the average value of a dataset. It is calculated by adding up all the values and dividing by the number of values. It's a common measure for normally distributed data. Because the calculation includes all values, including the outliers, the mean is sensitive to extreme values (outliers).

**Median**: The median is the middle value in a dataset when the values are arranged in ascending or descending order. It's less sensitive to outliers and skewed data than the mean. It provides a better measure of central tendency for skewed distributions or datasets with outliers.

**Mode**: The mode is the value that appears most frequently in a dataset. It's not affected by outliers. It's useful for categorical data and to identify the most common value in a dataset. Note that mode is not a good representation of the typical value because it's influenced by the frequency of the values.

## What is mean, median, variance, standard deviation, mode, range, interquartile range (IQR), and skewness?

These are the most important concepts in statistics.

### Mean

The mean, also known as the average, is a measure of the central tendency<a href="#what-is-central-tendency-in-math"><sup id="back-to-1">CT</sup></a> of a dataset. It's calculated by adding up all the values and dividing by the number of values. 

The formula is: 

```text
Mean = (Sum of all values) / (Number of values)
```

For example, if we have the values 2, 4, 6, 8, and 10, the mean would be:

Mean = (2 + 4 + 6 + 8 + 10) / 5 = 30 / 5 = 6

### Median

The median is another measure of central tendency<a href="#what-is-central-tendency-in-math"><sup id="back-to-1">CT</sup></a>. It's the middle value of a dataset when it's sorted in order. If the dataset has an odd number of values, the median is the middle value. If the dataset has an even number of values, the median is the average of the two middle values.

For example, if we have the values 1, 3, 5, 7, 9, the median would be 5.

### Variance

The variance is a measure of how spread out a dataset is. It's calculated by subtracting the mean from each value, squaring the result, and then averaging those squared differences.

```text
Variance = (Sum of (xi - mean)^2) / (Number of values - 1)
```

where `xi` is each value in the dataset.

For example, if we have the values 2, 4, 6, 8, 10, the variance would be:

Variance = ((2 - 6)^2 + (4 - 6)^2 + (6 - 6)^2 + (8 - 6)^2 + (10 - 6)^2) / (5 - 1) = 8 / 4 = 2

### Standard deviation

The standard deviation is the square root of the variance. It's a measure of how spread out a dataset is, but it's on the same scale as the original values.

```text
Standard Deviation = √Variance
```

For example, if the variance is 2, the standard deviation would be: 

Standard Deviation = √2 ≈ 1.41

### Mode

The mode is the value that appears most frequently in a dataset.

Consider the following set of numbers: 2, 4, 4, 4, 5, 6, 6, 7

To find the mode:

- Count the frequency of each number:
  - 2 appears 1 time
  - 4 appears 3 times
  - 5 appears 1 time
  - 6 appears 2 times
  - 7 appears 1 time

- Identify the number with the highest frequency:
  - The number 4 appears the most frequently (3 times).

So, the mode of this data set is 4, as it is the number that occurs most often.

### Range

The range is the difference between the largest and smallest values in a dataset. The range is a measure of how spread out the values in a data set are. It is calculated by subtracting the smallest value from the largest value.

Consider the following set of numbers: 3, 7, 2, 9, 5

To find the range:

1.	Identify the smallest value: The smallest value is 2.
2.	Identify the largest value: The largest value is 9.
3.	Subtract the smallest value from the largest value: Range = Largest value - Smallest value = 9 - 2 = 7

So, the range of this data set is 7.

The range provides a simple way to understand the spread of the data by showing the difference between the highest and lowest values.

### Interquartile range (IQR)

Interquartile range is a way to measure how spread out the middle part of a set of numbers is. It helps us understand the range where the middle half of the numbers lie, ignoring the highest and lowest values which might be outliers. It's widely used in statistics and data analysis to understand the spread and variability of a data set.

Explanation:

1.	Imagine we have a list of numbers: Like scores from a test.
2.	Arrange the numbers in order: From smallest to largest.
3.	Find the middle number: This is called the "median". If there are an odd number of numbers, the median is the one right in the middle. If there are an even number, it’s the average of the two middle numbers.
4.	Split the list into two halves:
   - The lower half (below the median).
   - The upper half (above the median).
5.	Find the median of the lower half: This is called the first quartile (Q1).
6.	Find the median of the upper half: This is called the third quartile (Q3).
7.	Calculate the IQR: Subtract the first quartile (Q1) from the third quartile (Q3).

Example:

Let’s say we have these test scores: 70, 55, 90, 65, 60, 75, 80, 95, 85  

1.	Arrange the scores: 55, 60, 65, 70, 75, 80, 85, 90, 95
2.	Find the overall median: 75 (the middle score).
3.	Split into lower and upper halves:
   - Lower half: 55, 60, 65, 70
   - Upper half: 80, 85, 90, 95
4.	Find Q1 (median of lower half): 62.5 (average of 60 and 65).
5.	Find Q3 (median of upper half): 87.5 (average of 85 and 90).
6.	Calculate the IQR: Q3 - Q1 = 87.5 - 62.5 = 25

So, the IQR is 25, meaning the middle 50% of the test scores lie within a range of 25 points. This helps us understand the spread of the main body of scores, ignoring the extreme highs and lows.

Uses of IQR:
1.	**Identifying outliers**
2. **Summarizing data**: Summary of the middle 50% of the data, offering a sense of where the bulk of the values lie. This is particularly useful for skewed distributions where the mean might not provide a clear picture of the central tendency<a href="#what-is-central-tendency-in-math"><sup id="back-to-1">CT</sup></a>.
3. **Comparing variability**: IQR is used to compare the spread of different data sets. By comparing the IQRs, we can understand which data set has more variability in the middle 50% of its values.
4. **Robust measure of spread**: IQR is a robust measure of statistical dispersion because it is not affected by extreme values or outliers. This makes it more reliable for understanding the spread of data than the range or the standard deviation in cases where the data contains outliers.

In summary, the IQR is a valuable tool in statistics and data analysis for understanding and summarizing the spread of a data set, especially when dealing with outliers and skewed distributions.

### Skewness

Skewness describes how a set of numbers is not evenly distributed around the average (mean). It tells us if the numbers are leaning more to one side of the average or if they are balanced. 

Types of skewness:

1. **Positive skewness** (Right-Skewed): Most numbers are smaller, but a few are much larger. For example, imagine a neighborhood where most houses cost between USD 100,000 and USD 300,000, but a few mansions cost over USD 1,000,000. Most house prices are on the lower side, but those few very expensive houses make the distribution lean to the right.
2. **Negative skewness** (Left-Skewed): Most numbers are larger, but a few are much smaller. For example, Think about a class where most students score between 70 and 100 on a test, but a few students score below 40. Most scores are on the higher side, but those few very low scores make the distribution lean to the left.
3. **Zero skewness** (Symmetrical Distribution): Numbers are evenly spread around the average. For example, if we roll a fair die many times, we would expect each number (1 through 6) to come up about the same number of times. The distribution of outcomes is balanced.

Why it matters?
- **Understanding data**: Skewness helps us see if the data is balanced or if there are extreme values that are pulling the average in one direction. For instance, in a business, if sales data is positively skewed, it means there are a few very high sales that are significantly affecting the overall average.
- **Making decisions**: Knowing about skewness can help in decision-making. For example, if a school notices that test scores are negatively skewed, they might want to provide extra help to the few students with very low scores.

In simple terms, skewness tells us if our data has a lean or tilt towards higher or lower values, or if it is evenly spread out. This can be very helpful in understanding and interpreting the data better.

## How do we interpret the standard deviation in the context of data variability?
The standard deviation (SD) is a measure of the amount of variation of a set of values from the average value (mean). A small SD suggests that the data points are in close proximity to the mean value, with minimal variation or dispersion. This indicates that the values are consistently similar and predictable. A large SD suggests that the data points span a broader range of values, which indicates the values are more dispersed, less consistent, and unpredictable.

## Explain the significance of the interquartile range (IQR) and how it is used to detect outliers?
IQR is a measure of the spread in the middle of a dataset. It's the distance between the first quartile (Q1) and the third quartile (Q3). It minimizes the impact of outliers by providing a comprehensive understanding of the spread and variability of the central portion of the dataset.

Here are steps to detect outliers using the IQR:

1. Sort the data: Arrange our data set in ascending order.
2. Locate the Q1 and Q3 quarters: Q1 is the median of the first half of the data (the 25th percentile), and Q2 is the median of the second half of the data (the 75th percentile).
3. Calculate the IQR: Subtract Q1 from Q3.
4. Determine the lower and upper bounds for outliers. To find the lower bound, subtract 1.5 times the IQR from Q1. To determine the upper bound, add 1.5 times the IQR to Q3.
5. Identify outliers. Any data point that is below the lower bound or above the upper bound is considered an outlier.

## What is a probability distribution?

A probability distribution is a mathematical function that describes the likelihood of different outcomes in a random experiment, such as tossing a coin or rolling a die. It describes the likelihood of various outcomes, which can include the possibility of the same result occurring multiple times. These likelihoods are often uncertain or probabilistic.

For example, let's say we're flipping a fair coin. The probability distribution of the outcome might look like this:

- Heads: 0.5 (or 50%)
- Tails: 0.5 (or 50%)

In this case, we can't predict the outcome with certainty because both heads and tails have the same probability of occurring (0.5 or 50%). The probability distribution tells us that the outcome is uncertain, and we can't know for sure what will happen.

It's important to note that a probability distribution means we can predict the likelihood of various outcomes, but we cannot predict the specific outcome of a single trial or event with certainty.

So, to summarize: The probability distribution provides a way to measure the uncertainty or randomness of an event or phenomenon, and we can use this information to make informed decisions or predictions.

## What is independent and identically distributed (i.i.d. or iid or IID)?

In probability theory and statistics, a collection of random variables is said to be independent and identically distributed (IID) if:

1. **Each random variable has the same probability distribution as the others**: 
   - This means that every random variable in the collection follows the same *probability distribution*. In other words, they have the same statistical properties such as mean, variance, and distribution shape.
   - **Example**: Consider flipping a fair coin multiple times. Let $ X_1, X_2, \ldots, X_n $ represent the outcomes of each flip, where $ X_i = 1 $ if the flip is heads and $ X_i = 0 $ if the flip is tails. Each $ X_i $ follows the same Bernoulli distribution with $ P(X_i = 1) = 0.5 $ and $ P(X_i = 0) = 0.5 $.
2. **All random variables are mutually independent**:
   - This means that the outcome of any one random variable does not affect the outcome of any other random variable in the collection. There is no dependency between the random variables.
   - **Example**: Continuing with the coin flip example, the outcome of the first flip $ (X_1) $ does not influence the outcome of the second flip $ (X_2) $, and so on. Each flip is an independent event.

## What is parametric estimation?

Parametric estimation is a statistical technique used to estimate the value of a parameter<a href="#ref-1" class="reference-link" data-ref="ref-1"><sup id="back-to-1">1</sup></a> (a number or a value) that describes a population<a href="#ref-3" class="reference-link" data-ref="ref-3"><sup id="back-to-2">3</sup></a> or a process. In simple terms, it's like trying to figure out the exact value of a mysterious number that controls a certain phenomenon.

Imagine we're trying to estimate the average height of a group of people. We take a random sample of 100 people and measure their heights. We then use this data to estimate the average height of the entire group. This is an example of parametric estimation. In this case, the parameter is the average height of the group, and the data is the heights of the 100 people in the sample. The goal is to use the data to estimate the value of the parameter, which is the average height of the entire group.

Parametric estimation is different from non-parametric estimation, which is a technique used when we don't know the exact form of the distribution of the data. In parametric estimation, we assume that the data follows a specific distribution (such as a normal distribution), and then use the data to estimate the parameters of that distribution.

Here are some key points to keep in mind:

- Parametric estimation is used to estimate the value of a parameter that describes a population or a process.
- The data is used to estimate the value of the parameter.
- The goal is to use the data to make inferences about the population or process.
- Parametric estimation assumes that the data follows a specific distribution (such as a normal distribution).

## How do maximum likelihood estimators (MLE) work?
MLE are a type of statistical estimator that is used to estimate the parameters of a probability distribution. Parameters here are the numerical values that define the specific characteristics of a probability distribution. MLE works by finding the parameter values that make the observed data most likely to occur.

Here's how MLE works:

1. Choose a probability distribution that we think best describes the data.
2. Identify the parameters of the probability distribution that we want to estimate. These could be the mean, variance, or other parameters that describe the distribution.
3. Write the likelihood function that calculates the probability of observing the data we have, given the values of the parameters we're trying to estimate. It's like a recipe that helps us estimate the amounts of ingredients we need to make the perfect food item, by calculating the probability of getting the exact food item we want.
4. Find the values of the parameters that make the likelihood function as high as possible.

Overall, MLE is a powerful tool for estimating the parameters of a probability distribution, but it requires careful consideration of the assumptions and limitations.

## What is empirical data?

Empirical data in mathematics refers to observations or measurements that are collected from the real world or from experiments. In other words, empirical data is the concrete evidence that helps us determine if a mathematical theory is correct or not. It's the "real-world" proof that backs up our mathematical ideas.

For example, let's say you're trying to develop a mathematical model to predict the spread of a disease. You collect data on the number of people infected, the rate of transmission, and other relevant factors. This data is the empirical evidence that helps you test and refine your model, making it more accurate and reliable.

So, in short, empirical data is the "real-world" evidence that helps us validate or refine our mathematical theories and models.

<div class="references">
   <hr>
   <h2>Notes and references</h2>
   <ol>
      <!-- <li>Nil</li> -->
      <li id="ref-1">1. <strong>Parameter</strong>: A parameter in estimation theory refers to a characteristic or attribute of a system or process under study or modeling. Parameters are typically numerical values that describe the behavior, properties, or relationships of the system. Parameters are fixed and often unknown values that describe a certain aspect of the population. Consider them as the "hidden" or "latent" variables that control the system's behavior. Examples of parameters in different fields include mean (µ) (the average value of a population), variance (σ²) (the measure of variability or spread in a population), etc. In estimation theory, the goal is to estimate the values of these parameters from a set of observed data, often in the presence of noise or uncertainty. We can then use the estimated values of the parameters to make predictions, simulate system behavior, or guide decision-making. <a href="#back-to-1" class="back-to-note">↩</a>
      </li>
      <li id="ref-2">2. <strong>Educated guess</strong>: An educated guess refers to a prediction or estimate that is based on a combination of data (historical data, other relevant information, etc.), knowledge (expertise, understanding of the underlying system or process, etc.), and assumptions (Logical and reasonable assumptions about the behavior of the system or process). An educated guess is not a wild or arbitrary guess, but rather a thoughtful and informed prediction that takes into account the available data and knowledge. It's a careful estimate that is based on a thorough understanding of the underlying system or process. <a href="#back-to-2" class="back-to-note">↩</a>
      </li>
      <li id="ref-3">3. <strong>Population</strong>: In the context of parametric estimation, the population refers to the entire group or set of data that we're trying to make inferences about. For example, if we're trying to estimate the average height of a group of people, the population would be the entire group of people, not just the 100 people in our sample. In other words, the population is the entire universe of data that we're interested in, and the sample is a smaller subset of that universe that we're using to make inferences about the population. Here are some examples of populations: A population of students in a school, a population of people in a city, etc. <a href="#back-to-3" class="back-to-note">↩</a>
      </li>     
   </ol>
</div>
